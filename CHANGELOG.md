<!-- # Changelog

All notable changes to the farmerchat-prompts project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [0.1.0] - 2024-12-15

### Added
- Initial release of farmerchat-prompts package
- Support for 3 AI providers: OpenAI, Claude (Anthropic), and Llama (Meta)
- 5 agricultural use cases:
  - Crop Recommendation
  - Pest Management
  - Soil Analysis
  - Weather Advisory
  - Market Insights
- PromptManager class for centralized prompt management
- Provider-specific prompt engineering optimizations:
  - OpenAI: Clear, structured instructions with role-based prompts
  - Claude: XML-tagged sections with detailed context
  - Llama: Direct instructions with example-based learning
- Pydantic models for type safety and validation
- Comprehensive test suite with 22 test cases
- Full documentation including:
  - README with usage examples
  - Quick Start guide
  - Publishing guide for PyPI
  - Example scripts
- Features:
  - Get prompts by provider and use case
  - Search prompts by keyword
  - List and validate prompt combinations
  - Format prompts with custom variables
  - Get provider-specific API structures

### Technical Details
- Python 3.8+ support
- Pydantic 2.0+ for data validation
- 15 total prompt variations (3 providers Ã— 5 use cases)
- All prompts follow prompt engineering best practices for their respective models
- Agricultural domain expertise integrated into all prompts

[0.1.0] -->


# Changelog

All notable changes to the farmerchat-prompts project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [0.2.0] - 2025-12-16

### Added - Multi-Domain Architecture

**New Domain System**
- âœ¨ Multi-domain architecture with explicit domain parameters
- âœ¨ New `Domain` enum: `CROP_ADVISORY`, `PROMPT_EVALS`
- âœ¨ Domain-specific prompt organization in subdirectories
- âœ¨ Backward compatible domain parameter (defaults to `crop_advisory`)

**Prompt Evaluation Domain (prompt_evals)**
- âœ¨ **Specificity Evaluator**: Classifies agricultural facts as Specific/Not Specific based on 7 evaluation flags (entity, location, time, quantity, conditionality, mechanism, actionability)
- âœ¨ **Fact Generator**: Extracts atomic, verifiable facts from chatbot responses with Bihar-specific context, confidence scoring, and strict exclusion criteria for conversational elements
- âœ¨ **Fact Matcher**: Performs semantic matching between predicted and ground truth facts with agricultural context awareness and confidence scoring
- âœ¨ **Contradiction Detector**: Identifies contradictions with component-level analysis (temperature, humidity, timing, quantity) and structured justifications
- âœ¨ **Relevance Evaluator**: Multi-dimensional fact evaluation with scores for relevance, ground truth alignment, practical value, specificity, and agricultural soundness

**New Features**
- âœ¨ 5 new use cases in `UseCase` enum for prompt evaluation
- âœ¨ Support for additional parameters and context in all prompt_evals prompts
- âœ¨ Complete evaluation pipeline with 6-step workflow
- âœ¨ Comprehensive expected outputs and application sequences
- âœ¨ Integration patterns with eval.py and production systems

**API Enhancements**
- âœ¨ `get_prompts_by_domain(domain)`: Get all prompts in a specific domain
- âœ¨ `get_available_domains()`: List all available domains
- âœ¨ Domain parameter in `get_prompt()` method
- âœ¨ Enhanced stats with domain count

**Documentation**
- ðŸ“š Complete application sequence guide for evaluation pipeline
- ðŸ“š Expected input/output examples for all prompt_evals use cases
- ðŸ“š "Why These Prompts Matter" section explaining importance
- ðŸ“š Step-by-step pipeline implementation with code
- ðŸ“š Timeline estimates and use case scenarios
- ðŸ“š Enhanced README with prompt evaluation deep-dive
- ðŸ“š `MULTI_DOMAIN_MIGRATION.md`: Complete migration guide
- ðŸ“š `COLAB_TESTING_GUIDE.md`: Google Colab testing notebook
- ðŸ“š `GITHUB_PUBLISHING_GUIDE.md`: GitHub publishing instructions
- ðŸ“š Updated `QUICKSTART.md` with domain examples
- ðŸ“š Updated `usage_examples.py` with 13 examples (was 10)

**Testing**
- âœ… 11 new test cases for domain functionality
- âœ… 6 new test cases for prompt_evals validation
- âœ… Total test coverage: 33 tests (was 22)
- âœ… New test classes: `TestDomainSupport`, `TestPromptEvalsDomain`

### Changed

**Package Structure**
- ðŸ”„ Reorganized `prompts/` directory into domain subdirectories
- ðŸ”„ Moved existing crop advisory prompts to `prompts/crop_advisory/`
- ðŸ”„ Added new `prompts/prompt_evals/` directory
- ðŸ”„ Updated import paths from `..models` to `...models`
- ðŸ”„ Enhanced `PromptMetadata` with domain field

**API Changes (Backward Compatible)**
- ðŸ”„ `get_prompt()` now accepts optional `domain` parameter
- ðŸ”„ Internal storage structure changed to three-level nesting: `{provider: {domain: {use_case: Prompt}}}`
- ðŸ”„ All existing API calls work without modification (default to `crop_advisory`)

**Prompts**
- ðŸ”„ All crop advisory prompts updated with `domain=Domain.CROP_ADVISORY` in metadata
- ðŸ”„ All prompt_evals prompts include support for `additional_params` and context variables

### Technical Details

**New Classes & Enums**
```python
class Domain(str, Enum):
    CROP_ADVISORY = "crop_advisory"
    PROMPT_EVALS = "prompt_evals"

class UseCase(str, Enum):
    # Existing crop advisory
    CROP_RECOMMENDATION = "crop_recommendation"
    PEST_MANAGEMENT = "pest_management"
    SOIL_ANALYSIS = "soil_analysis"
    WEATHER_ADVISORY = "weather_advisory"
    MARKET_INSIGHTS = "market_insights"
    
    # New prompt evaluation
    SPECIFICITY_EVALUATION = "specificity_evaluation"
    FACT_GENERATION = "fact_generation"
    FACT_MATCHING = "fact_matching"
    CONTRADICTION_DETECTION = "contradiction_detection"
    RELEVANCE_EVALUATION = "relevance_evaluation"
```

**Prompt Engineering**
- All prompt_evals prompts follow OpenAI best practices for JSON output
- System prompts range from 200-800 words with clear instructions
- Comprehensive examples and evaluation frameworks included
- Support for agricultural context and Bihar-specific considerations

**Implementation Stats**
- Total prompts: 20 (15 crop advisory + 5 prompt_evals)
- New code: ~2,500+ lines
- New documentation: ~1,500+ lines
- Test coverage: 50% increase

### Migration Notes

**For Existing Users**
- No breaking changes - all existing code continues to work
- Domain parameter is optional and defaults to `crop_advisory`
- No action required unless using new prompt_evals domain

**For New Domain Usage**
```python
# Old API (still works)
prompt = manager.get_prompt("openai", "crop_recommendation")

# New API (explicit domain)
prompt = manager.get_prompt("openai", "crop_recommendation", "crop_advisory")
prompt = manager.get_prompt("openai", "specificity_evaluation", "prompt_evals")
```

**Migration Path**
1. Update `models.py` - Add `Domain` enum
2. Update `manager.py` - Add domain parameter
3. Reorganize `prompts/` directory structure
4. Update import paths in moved files
5. Add domain field to existing prompt metadata
6. Run tests to verify backward compatibility

See `MULTI_DOMAIN_MIGRATION.md` for complete step-by-step guide.

### Performance

**Evaluation Pipeline Benchmarks**
- Fact Generation: ~2-5 seconds
- Specificity Check: ~1-2 seconds per fact
- Fact Matching: ~2-3 seconds per ground truth fact
- Contradiction Detection: ~3-5 seconds per check
- Relevance Evaluation: ~5-10 seconds (batch)
- Total pipeline: ~30-60 seconds for 5-10 facts

**API Call Estimates**
- Single fact evaluation: 1-2 API calls
- Complete pipeline (5 facts): ~15-20 API calls
- Estimated cost (GPT-4): ~$0.10-0.20 per response

### Breaking Changes

**None** - This release is fully backward compatible.

### Deprecations

**None**

### Security

- No security-related changes in this release

### Dependencies

**No new dependencies added**
- Still requires only: `pydantic>=2.0.0`

---

## [0.1.0] - 2024-12-15

### Added - Initial Release

**Core Features**
- âœ¨ Multi-provider support: OpenAI, Claude (Anthropic), Llama (Meta)
- âœ¨ 5 agricultural use cases (crop advisory domain):
  - Crop Recommendation
  - Pest Management
  - Soil Analysis
  - Weather Advisory
  - Market Insights
- âœ¨ 15 total prompt variations (3 providers Ã— 5 use cases)
- âœ¨ Provider-specific prompt engineering optimizations

**Package Architecture**
- ðŸ“¦ PromptManager class for centralized prompt management
- ðŸ“¦ Pydantic models for type safety and validation
- ðŸ“¦ Clean API: `get_prompt(provider, use_case)`
- ðŸ“¦ Comprehensive test suite with 22 test cases

**Prompt Engineering**
- ðŸŽ¯ OpenAI: Clear, structured instructions (200-500 words)
- ðŸŽ¯ Claude: XML-tagged sections with detailed context (500-1000+ words)
- ðŸŽ¯ Llama: Example-driven, direct instructions (300-800 words)

**Documentation**
- ðŸ“š Complete README with usage examples
- ðŸ“š Quick Start guide
- ðŸ“š Publishing guide for PyPI
- ðŸ“š Example scripts with 10 practical examples
- ðŸ“š Comprehensive inline documentation

**Features**
- âœ… Get prompts by provider and use case
- âœ… Search prompts by keyword
- âœ… List and validate prompt combinations
- âœ… Format prompts with custom variables
- âœ… Get provider-specific API structures
- âœ… Type-safe operations with Pydantic

**Testing**
- âœ… 22 test cases covering all functionality
- âœ… Provider-specific prompt validation
- âœ… Coverage for all use cases
- âœ… Edge case handling

**Technical Details**
- Python 3.8+ support
- Pydantic 2.0+ for data validation
- MIT License
- Professional packaging for PyPI

---

## Version Comparison

| Feature | v0.1.0 | v0.2.0 |
|---------|--------|--------|
| Providers | 3 | 3 |
| Domains | 1 (implicit) | 2 (explicit) |
| Use Cases | 5 | 10 |
| Total Prompts | 15 | 20 |
| Test Cases | 22 | 33 |
| API Methods | 8 | 10 |
| Documentation Files | 5 | 10 |
| Examples | 10 | 13 |
| Code Lines | ~3,000 | ~5,500 |

---

## Links

- **Repository**: https://github.com/aakashdg/farmerchat-prompts
- **Documentation**: See README.md
- **Issues**: https://github.com/aakashdg/farmerchat-prompts/issues
- **PyPI**: (Coming soon)

---

**Note**: This is a major feature release (0.1.0 â†’ 0.2.0) that maintains full backward compatibility while adding significant new functionality for prompt evaluation and quality assessment.
